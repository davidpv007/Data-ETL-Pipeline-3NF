# LDJ - Data Jobs ETL Pipeline

A data pipeline that ingests job posting data from CSV, transforms it into a normalized 3NF relational model, and loads it into PostgreSQL.

## Project Structure

```
â”œâ”€â”€ README.md
â”œâ”€â”€ airflow
â”‚   â”œâ”€â”€ config
â”‚   â”œâ”€â”€ dags
â”‚   â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â””â”€â”€ data_jobs_pipeline.py
â”‚   â”œâ”€â”€ logs
â”‚   â”‚   â”œâ”€â”€ dag_id=data_jobs_pipeline
â”‚   â”‚   â”œâ”€â”€ dag_processor_manager
â”‚   â”‚   â””â”€â”€ scheduler
â”‚   â””â”€â”€ plugins
â”œâ”€â”€ credentials.json
â”œâ”€â”€ data
â”‚   â””â”€â”€ data_jobs.csv
â”œâ”€â”€ data_ingestion.sql
â”œâ”€â”€ docker
â”‚   â”œâ”€â”€ docker
â”‚   â”‚   â””â”€â”€ init-multiple-dbs.sh
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ init-multiple-dbs.sh
â”œâ”€â”€ notebooks
â”‚   â””â”€â”€ data_pipeline.ipynb
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ schema
â”‚   â”œâ”€â”€ er_diagram.dbml
â”‚   â””â”€â”€ schema.sql
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â”œâ”€â”€ __init__.cpython-313.pyc
â”‚   â”‚   â”œâ”€â”€ config.cpython-313.pyc
â”‚   â”‚   â”œâ”€â”€ data_pipeline.cpython-313.pyc
â”‚   â”‚   â””â”€â”€ path.cpython-313.pyc
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ data_pipeline.py
â”‚   â”œâ”€â”€ ingestion.py
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ transformation.py
â”‚   â””â”€â”€ utils.py
â””â”€â”€ tests
    â””â”€â”€ __init__.py
```

## Setup

### Prerequisites

- Python 3.13+
- Docker and Docker Compose
- Apache Airflow

### 1. Environment Setup

```bash
# Navigate to project directory
cd ldj

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Database Setup

```bash
# Start PostgreSQL container
docker compose up -d

# Verify container is running
docker compose ps
```

### 3. Configure Environment

Copy `.env.example` to `.env` and update credentials if needed:

```bash
cp .env.example .env
```

## Running the Pipeline

Launch the python file for running the ETL

```bash
data_pipeline.py
```

## Running Tests

```bash
pytest tests/ -v
```

## Database Schema

The pipeline transforms flat CSV data into a normalized 3NF model with the following tables:

## ðŸ“¦ Database Schema (3NF)

The pipeline normalizes the raw CSV into a 3NF relational model.  
The following tables are created and populated by the ETL:

### Dimension Tables
- `companies` â€” Unique companies that posted jobs
- `locations` â€” Geographic job location dimension
- `platforms` â€” Job posting platform dimension
- `schedule_types` â€” Work schedule type dimension (e.g. full-time, contract)
- `skill_types` â€” Skill category/type dimension
- `skills` â€” Individual skills dimension (e.g. python, aws)

### Fact Table
- `jobs` â€” Main job postings fact table (one row per job)

### Bridge / Junction Tables
- `bridge_job_skill` â€” Many-to-many relationship between `jobs` and `skills`
- `job_skills` â€” Staging exploded skill list prior to bridging

### Staging Layer
- `data_jobs_raw` â€” Cleaned staging data loaded from CSV before dimensional modeling

### ER Diagram
See: `schema/er_diagram.dbml` for the complete diagram and relationships.

See `schema/er_diagram.dbml` for the complete ER diagram.

